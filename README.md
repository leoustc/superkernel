# SuperKernel

SuperKernel is a custom Jupyter kernel that brings supercomputing to your desktop ‚Äî **without adding AI infrastructure complexity**.  
Run notebook cells across many GPUs and nodes via PyTorch Distributed (TorchRun). Perfect for AI, ML, and HPC at any scale.

![Super Kernel](superkernel.png)

SuperKernel unifies scale-up networking (NVLink, PCIe, CXL) with scale-out networking (RDMA, InfiniBand, Ethernet), making them work together as if they were one single kernel. Inspired by NVIDIA‚Äôs SuperPOD, SuperKernel transforms the notebook into a gateway to supercluster-class computing.

---

## Why SuperKernel?

- üñ•Ô∏è **Stay in Jupyter** ‚Äî zero workflow change.
- ‚ö° **Scale to clusters** ‚Äî execute one cell on thousands of GPUs.
- üîó **Scale-up + Scale-out unified** ‚Äî intra-node and inter-node networking exposed as one kernel abstraction.
- ü§ñ **AI/ML native** ‚Äî debug collectives, shard data, inspect per-rank state.
- üî¨ **Transparent** ‚Äî per-rank outputs merged back into a single cell.
- üåê **Elastic** ‚Äî provision instantly, scale elastically, and destroy completely when done.
- üö´ **No more AI infra complexity** ‚Äî scale with boundless simplicity, no new tools or platforms to learn.
- ‚ôæÔ∏è **Boundless scale** ‚Äî from your laptop to the world‚Äôs largest clusters, all from your notebook.

---

## Vision

> ‚ÄúOne kernel, infinite scale ‚Äî SuperKernel unifies scale-up and scale-out networking into a single execution space, unlocking AI/ML at supercluster scale with a single click. No more infrastructure headaches, just code and scale.‚Äù

SuperKernel = Scale-Up + Scale-Out ‚Üí One Kernel

Just as NVIDIA‚Äôs SuperPOD showed how GPUs can be assembled into a supercomputer, SuperKernel shows how your notebook can become the control plane of a supercluster.


![Super Kernel Demo](superkernel.gif)


---

## Examples

See `examples.ipynb` for runnable cells that demonstrate:
1. **Cluster info** ‚Äî `%info` magic shows world size, CUDA, devices.
2. **Multi-host shell** ‚Äî run `!hostname` on every rank and merge results.
3. **All-Reduce** ‚Äî interactive collective across ranks.
4. **Data sharding** ‚Äî split a dataset by rank inside a single cell.
5. **Barrier timing** ‚Äî measure sync latencies across the cluster.

---

## Learn More

- PyPI: (coming soon: superkernel)
- Backend: PyTorch Distributed
- Inspiration: NVIDIA SuperPOD

---

## Further Readings

To dive deeper into the ideas behind SuperKernel:
- [Distributed PyTorch (TorchRun)](https://pytorch.org/docs/stable/elastic/run.html)
- [NVIDIA NVLink and NVSwitch (scale-up networking)](https://www.nvidia.com/en-us/data-center/nvlink/)
- [RDMA and InfiniBand (scale-out networking)](https://community.mellanox.com/s/article/what-is-rdma-x)
- [NVIDIA SuperPOD ‚Äî inspiration for cluster-scale AI infrastructure](https://www.nvidia.com/en-us/data-center/superpod/)

---

SuperKernel:  

**No more AI infra complexity. Boundless scale. All from your notebook.**
